1:15pm. Putting the lambda function PHI back into airsim env because I think that the depth planner image was messing things up. By this, I mean that the values would vary wildly. This is because the pixel values are the distances (in m) from the car. So, you could imagine that if a pixel is pointed at the sky that would cause some issues. PHI should help to down play that issue. I want to avoid batch normalization as much as possible.

1:31pm. Well, it kind of worked. Note, I also reduced the learning rate from 0.0007 to 0.0007, and I also reduced the number of filters per layer to 64 from 96. Now at 4.5M params instead of 9M params. I wanted to prioritize training time. ANYWAYS...the lambda function seems to be helping. It has at least gotten the Q values to the point that they are no longer stationary, so that's good. Now, though, the Q-values are like 20,000 to 50,000, which is crazy! These were randomly init'd weights, so it's not a leftover from the previous iteration of training. Maybe the learning rate is too low now? I don't think so because the learning rate 0.0007 was getting stationary Q-values and now, t0.0001 is not, so I guess it's all alright? I think, though, that this is what we want: a slow convergence to the optimal weights. It's just that it's going to take SO long to get there. I guess I can consider the stationary Q-values problem solved?


1:40pm. So in testing, it only wants to turn left, and the Q values increase to 80k when it sees the grass and the trees. I guess batch normalization might really be necessary. Adding that now.

1:56pm. Added batch normalization between the conv layer and the activation layer (as https://arxiv.org/pdf/1502.03167.pdf). I also am not using tanh instead of relu as activation of fully conected/dense layers. Immediatley, the Q-values are already around range [-1, 1], so it seems like this might actually work! I also just increased the learning rate to 0.001 because the paper recommended a higher learning rate, and it also was able to learn weights 14x faster (for ImageNet?) with batch normalization than w/out batch normalization. Therefore, increasing lr by 10x should be safe. 

2:09pm. Training looks like its going really well. The Q-values seem to be somewhat reacting to whats on the screen. I've discovered a problem, though, when writing the recent_state images to file: the crop of the scene images is too low. In fact, it looks too flat. I think that this is a result of the FOV in settings.json being too high. Plus, the first 4 feet fakes up like 75% of the image, which makes scene images worthless (at least as far as i can tell). Chaning FOV for depth and scene to 90. Result: MUCH BETTER! Still a bit of tunnel vision, but at least the car should be able to see more than 8 feet ahead! Note: this may also have been contributing to the Q values becoming stationary since, as far as I could tell too, the images looked almost identical in the FOV=120 case.

2:28pm. Thank God for some progress at least. The Q-values, at the very least, seem to be responding somewhat to the input it receives, we aren't getting wacky Q-values, and I am more confident about the input to the neural network given that I've seen the image from recent_state in MDQN... .py. 

3:05pm. Chaning the learning rate to 0.00025 from 0.001 because whenever the network updates (every 6th iteration), for pretty much the same state, the Q-value will jump by 0.5. Given that the Q-values are [-2, 2] ish, this is bad. So, the updates to the weights of Q probably need to be refined? Or maybe I should just let it train? IDK. I'm going to lower the lr anyways. 

3:12pm. Note: during the initial 256 random steps (which are just to establish a base of transitions from which to sample), the Q-values varied between 1.0 and 1.1, which is fantastic news. This means that the network is responding to changes in input, and though the updates have been all over the place, the network is somewhat stable; it has learned something. Note: I probably trianed @ 0.001 lr for approximately 10k iterations, and now i've reduced lr to 0.00025, one-fourth of what it was. Now, updates to Q are 0.05 to 0.2 at a time, which is more so what I'm looking for. Perhaps this is evidence that I need to use a decaying learning rate?

3:56pm. Some notes about the training: it seems like the agent is learning not to swerve right immediately into the grass and rocks and trees, so that's good. Also, I noticed that the Q-values turned negative just before the vehicle collided with a person, so that's good.

4:48pm. It looks like the agent has made some progress, so I'm going to reduce the lr to lr= 0.0001. Note: it just trained for 20k iterations, so it SHOULD be learning something, which it seems to be doing. It's at least recognizing: "oh no! there's a building in front of me. Going that direction will probably result in a negative reward,'' or something. Same goes for people and a little bit for cars. The problem, however, seems to be that the agent is reacting too slowly. In other words, it is not identifying object quickly enough. MAYBE stacked states would help this, but I'm not so sure. It has the speed, its heading, its heading relative to the destination, and it has the depth image, so it should be avoiding these things ahead of time. Maybe it just needs more training time? IDK. That's why I reduced the learning rate, too: maybe it needs finer adjustments to figure this all out. Note: since I'm loading the previous training session/iteration's weights, me changing the learning rate is a lot like decaying the learning rate since the weights are not lost. 

5:36pm. Just uploaded my final draft for this semester btw. So, it seems like the agent can pick up on collisions about 1 second before the happen. At that point, the Q-values all seem to turn negative. So, it is at least saying: "oh no. there's a wall. It's very likely that I'm going to hit that wall." What it is not saying is: "oh, there's a wall off in the distance, and I'm headed that way. Perhaps I should adjust my course so as to avoid that wall." For the latter, I may need to introduce stacked states. And if that doesn't work, then I may need to use stacked states and recurrent layers instead of fully connected layers. Otherwise, thank you Lord for this progress. 

5:43pm. AirSim (UE4Game) just crashed again, so I'm reducing lr from lr= 0.0001 to lr= 0.00007 just because it's been another 10.7k iterations since last restart. I'll click "close program" and then let it run again at this learning rate while I go to dinner. 