6:15am. A few improvements were made. For one, PHI, which is applied to depth images as preprocessing function, is now vectorized. Not 100% sure what this means, but maybe it's something like it parallelizes it or it's a native for-loop for numpy (which written in C?). Point being: PHI is now literally 10x faster. Check scratch_work/get_... .py if you want to see the times of some other approaches (though, it's probably commented out w/ the """'s).----I am also now using a panoramic image of the front, ~180 degrees. I thought about how the car would confuse its starting point with the roundabout, and I realized that that was actually not such a bad thing; it was actually identifying a similar situation. So, my hope is, that with more information about its surroundings in the form of these composite/panoramic images, I think that the car should not suffer from the same problem it did last time AND still be able to learn similar situations.----Also, I am now using multi_input_processor instead of multiinput dqn because it is simpler. I kind of forgot about Processors in keras-rl. I couldn't use the default one because of a shape error. There were errors because keras-rl's dqn agent, for whatever eason, wraps the state_batch in another, superfluous array before passing it to batch_processor or something. There was also a discrepancy during training, but I that one was on me: I didn't iterate through the samples in the batch; I was just taking the first one?----Apples-to-apples, I have a faster PHI. This one is conditional, and its aim is to highlight objects within a few meters/feet (still not sure on the units here) in pure white, 255. Beyond this point is some inverse, e.g., 255/depth_planner_pixel_value. This should make depth images more useful since it should be able to much more easily identify imminent collisions, while still (hopefully) being able to pick-up on objects. There should be stark enough of a contrast between the pure white and everything else for there to be no confusion between safe and unsafe (b4 it might have been conflating the white road immediately in front of it with the white side of the building/car/person it was hitting; mixed signals, really. I have images and have updated airsim_env and keras_... to reflect these changes; everything at leasts runs without the Python interpreter throwing error messages. Oh! I also reduced the resolution to 256x256 for both and FOV is now 45 not 90. New orientations for cameras too: tilt down at like 10 degrees or something.