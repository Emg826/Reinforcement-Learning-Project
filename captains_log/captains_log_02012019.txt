502pm. So I did a lot of stuff today, but most of it I just dropped. Most of my work today was focused on getting the number of parameters in the network down and trying different camera angles and trying to figure out why the simulation started taking so long. In reverse order, the request for all of the sim images was taking between 0.1 and 0.3 seconds! Way too long! The only thing that reduced the wait time was to reduce the number of images; the dimensions of the image had no effect. Everything else in AirSimEnv, from the reshapes to the binary string to state operations all was comparably negligble. For now, I'm not going to do anything about it because I don't really know what to do. I suppose that I could do multiprocessing to split depth and scene images into 2 process, but IDK. 

Camera angles, well, I think I know why the nn didn't learn about curbs: when it is colliding with curbs, all that it sees are sidewalks. So, this leads me to try to find a new camera angle that still saw far enough out but would for-sure see the curb if it was colliding with it. Unfortunately, after a lot of time and adjustments to airsimevn, I couldn't come up with anything. So, for now, I guess that the settings.md is more or less fixed, as is the setup_my_cameras() function in airsimenv.

The new neural network now has 10M instead of 7M parameters. I know, I know, I said "reduce," but I was also worried that maybe the network just was not powerful enough as it was. So, I halved the size of the depth image. This allowed me to reduce the number of neurons in the depth conv layers and increase the number of neurons in the scene layers. I did add maxpooling, but it's only 2x2 pooling, so it shouldn't be too big of a deal. Still worried that it might make the network unable to determine direction, but oh well, we'll see. What else? Another fully connected layer, so that should be good. First 2 fully connecteds now have elu so as to allow for > 1 activation values while last has sigmoid so as to ensure outputs are in [0, 1], which matters for q-value predictions. I'm training from scratch now and I am using a higher initial learning rate of 0.01 with a decay lr / (100,000 / 6), so it should all come together. 


OK nvrmnd :( ... all but the 5th q-value predictions went to nan. Well, chaning the elu's back to sigmoids, then, i guess. So, I guess to conclude, I made the network wider for scene and fully conn'd, smaller for depth while also reducing depth image size, did some SGD stuff, and that's it. I'm going to let it train now. I probably should add canny image stuff, but I'm too tired rn.


955pm. Just implemented a proximity sensor. You can think of it as a histogram of the pixels in num_proximity_sensor_sector columns. The idea is that you set a proximity radius, a kill zone radius, and this produces a histogram based on the piecewise function in the class. It'll produce a 1D array rather than a 2D image, which may or may not help with driving. Idk, but I do have to say that the depth planner image at this low resolution (individual is 128x128) is not all that helpful. I'm going to let it train overnight, and we'll see. Oh, so the kill zone is zone @ which proximity value are set to 1. So the way the proximity sensor works is it receives a depth planner image chopped off accordingly. Then, it applies the vectorized piecewise function:   lambda depth_planner_pixel: self.max_proximity_value if depth_planner_pixel < self.kill_distance  \
                                                                                                                                        else (self.min_proximity_value if depth_planner_pixel > self.max_distance \
                                                                                                                                                                                else self.outside_kill_inside_max_proximity(depth_planner_pixel)), to each pixel in the image. Then, for each of the num_sectors approximately (give or take a column) equally sized sectors in depth image (sector=bunch of columns in proximity image / preprocessed depth image). Then, use np.sum() to sum up everything in the sector. Sectors w/ higher values than others have most stuff in close proximity. Not 100% sure that this'll help, but we'll see. Also, I changed up AirSimEnv a bit to accomodate proximitity sensor, but it should still be backward compatibile with other previous nn models (keras_....py changed too), but I haven't tested this, so I don't really know. Like I said, it could be a really bad idea, or it could turn out to be a more efficient neural network. Who knows?