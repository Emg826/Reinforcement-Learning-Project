908am. Fantastic: it drives in a damned circle!!!!!!! The 12 hours of training, the LSTM, and all of that shit, for nothing! Maybe not nothing. At least I now know that throwing an LSTM at it won't work. I suspect this has something to do with the reduction in the number of neurons, but I don't really know. 

I did find something interesting last night, though: CoordConvNets. The idea is behind the grayscale pixel values, to put the cartesian (x, y) coordinates of the pixel. Sounds like it wouldn't work, but the UberAI people found that this improved performance of the DQN on games like Ms. Pac Man. The reason performance improved is that supplying the Cartesian coordinates allows the network, if it so chooses, to lose the spatial invariance property, i.e., pixels in certain regions mean something different than if the were in another region. Now, I do have a reservation about this. Namely that Ms. Mac Man is a fixed board; driving is not a "fixed board." Therefore, would this hamper learning? I don't know, so I tink I'm going to try and implement it and find out. Also, apparently the paper used kernel=(1,1), which is terribly impractical for my purposes.

952am. So great news: I found code on github from titu1994 for CoordConvNet that just concatenates the i, j, r coordinates to the channel dimension for any given input. This is WAY easier than implementing this in AirSimEnv. Also, I added a print frequency to TransparentDQN so that the terminal isn't just bombarded with Q-values that don't really change much between any given iteration.