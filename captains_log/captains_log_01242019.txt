1111am. Ok, so it trained for approximately 50k+ iterations, which is a fantastic sign that disabling the error message popup worked! However, there were still 4 instance of the simlulation opened, so that probably dramatically effected performance. Perhaps I need to give it more time between killing the simulation and starting back up? I just changed the waits from 2 seconds are the terminate()'s to 15 and 4 for the sim and client respectively.

As for the performance of the car, it basically drives in a bunch of S's. It would either start to go around in a circle and then go the wrong way, or it would head towards the end of the roundabout but not make it past the 1/2 way point. So, I lowered the learning  rate from 0.0025 to 0.00025. Hopefully this helps. In the future, I should probably use a decaying learning rate. 

As far as what the car learned, well, it definitely learned to avoid some collisions. It can recognize fairly well in advance that: 'oh hey! there's a car there. i should swerve.' I am pleased with the progress. I still, however, think that the goal is too poorly defined for the driving agent to learn to accomplish its task. I still think that the episodes should be on mini-obstacle courses that are maybe 100-150 sim units long. Cycling through a bunch of different scenarios would probably help the model generalize, too. Also, I really need to speed up training time. I should be able to do something to get to 2x speed but w/ 2x number of steps in IRL time. I don't know how I'll do it, though. 


407 pm. It must've trained on at least 100k iterations by now and its still driving around in a circle! I made a child class of DQNAgent that prints out the q-values whenever something is passing through the network, and surprise surprise: the q-values were the same regardless of the input. Discouraging, I know. However, I'm going to remove batch normalization (probably messing up the pure white in depth image) and i'm going to add leaky relu instead of relu in case there are a lot of dead relus in the network rn. Oh, also, for RNN, I could probably just use a Conv3D and do (w, h, depth=1). That should preserve temporality. I'm also using SGD w/ init_lr =0.01 and decay  = 0.0001665...lr := lr * ( 1 / (1 + (decay * iterations)))